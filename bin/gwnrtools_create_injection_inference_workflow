#!/usr/bin/env python
#
# Copyright (C) 2020 Prayush Kumar
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

"""Setup workflow to perferm Bayesian parameter estimation runs on a
custom set of simulated signals"""

import os
import argparse
import logging
import shutil
import subprocess
import glob
import tempfile
import numpy

from glue.pipeline import CondorDAGNode, CondorDAG
import pycbc
from pycbc import fft, opt, scheme
from pycbc.workflow import configuration

from GWNRTools.DataAnalysis.MiscFunctions import get_unique_hex_tag
from GWNRTools.Workflow.CondorUtilities import BaseJob, InferenceJob
from GWNRTools import __version__


def get_ini_opts(confs, section):
    op_str = ""
    for opt in confs.options(section):
        val = confs.get(section, opt)
        op_str += "--" + opt + " " + val + " \\" + "\n"
    return op_str


def mkdir(dir_name):
    try:
        subprocess.call(["mkdir", "-p", dir_name])
    except OSError:
        pass


####
# InferenceAnalysis etc


class InjectionInferenceAnalysis():
    def __init__(self, opts, run_dir, config_files,
                 inj_exe_name='inspinj', inf_exe_name='inference',
                 plt_exe_name='plot',
                 verbose=False):
        '''
        Setup and run inference on one single injection

        Parameters
        ----------
        opts : ConfigParser object
            Configuration options
        run_dir : string
            Path to directory in which the analysis is to be run
        config_files : dict
            Dictionary with names and locations of ini files needed
        inj_exe_name : string
            Name of the injection exe's options' section in opts
        inf_exe_name : string
            Name of the inference exe's options' section in opts
        '''
        self.verbose = verbose
        self.opts = opts
        self.run_dir = run_dir
        self.config_files = config_files
        self.inj_exe_name = inj_exe_name
        self.inf_exe_name = inf_exe_name
        self.plt_exe_name = plt_exe_name

    def get_run_dir(self): return self.run_dir

    def get_opts(self): return self.opts

    def get_config_files(self): return self.config_files

    def get_inj_exe_name(self): return self.inj_exe_name

    def get_inj_exe_path(self):
        return os.path.join(self.run_dir, "make_injection")

    def get_inf_exe_name(self): return self.inf_exe_name

    def get_inf_exe_path(self):
        return os.path.join(self.run_dir, "run_inference")

    def get_plt_exe_path(self):
        return os.path.join(self.run_dir, "make_plot")

    def get_log_dir(self): return os.path.join(self.run_dir, 'log')

    def setup(self):
        # Make the analysis directory
        if self.verbose:
            logging.info("Making {0} in {1}".format(self.run_dir, os.getcwd()))
        mkdir(self.run_dir)
        mkdir(os.path.join(self.run_dir, 'scripts'))
        mkdir(os.path.join(self.run_dir, 'log'))
        mkdir(os.path.join(self.run_dir, 'plots'))

        # Copy over the relevant configuration files
        if self.verbose:
            logging.info("Copying config files to {0}".format(self.run_dir))
        for conf_name in self.config_files:
            shutil.copy(self.config_files[conf_name],
                        os.path.join(self.run_dir, '{0}.ini'.format(conf_name)))

        # Copy over executables
        if self.verbose:
            logging.info("Copying executables to {0}".format(
                os.path.join(self.run_dir, 'scripts/')))
        for _, exe in self.opts.items('executables'):
            shutil.copy(exe, os.path.join(self.run_dir, 'scripts/'))
            os.chmod(os.path.join(self.run_dir, 'scripts/',
                                  os.path.basename(exe)), 0o0777)

        # Write injection creation script
        self.opts.set(self.inj_exe_name, 'ninjections', '1')
        self.opts.set(self.inj_exe_name, 'seed',
                      str(numpy.random.randint(1, 1e5)))
        self.write_run_script(self.opts.items(self.inj_exe_name),
                              "scripts/{0}".format(os.path.basename(
                                  self.opts.get('executables', self.inj_exe_name))),
                              os.path.join(
                                  self.run_dir, "make_injection"))

        # Write pycbc_inference run script
        self.write_run_script(self.opts.items(self.inf_exe_name),
                              "scripts/{0}".format(os.path.basename(
                                  self.opts.get('executables', self.inf_exe_name))),
                              os.path.join(
                                  self.run_dir, "run_inference"),
                              """#!/bin/bash

# run sampler
# Running with OMP_NUM_THREADS=1 stops lalsimulation
# from spawning multiple jobs that would otherwise be used
# by pycbc_inference and cause a reduced runtime.
OMP_NUM_THREADS=1 \\\n""")

        # Write pycbc_plot_posterior run script
        if self.opts.has_option('executables', self.plt_exe_name):
            self.write_run_script(self.opts.items(self.plt_exe_name),
                                  "scripts/{0}".format(os.path.basename(
                                      self.opts.get('executables', self.plt_exe_name))),
                                  os.path.join(self.run_dir, "make_plot"))

    def write_run_script(self, exe_opts, exe_path, script_name,
                         script_base="""#!/bin/bash\n"""):
        out_str = script_base
        out_str += "{0} \\\n".format(exe_path)
        for exe_opt_name, exe_opt in exe_opts:
            out_str += "  --" + exe_opt_name + " " + exe_opt + " \\\n"
        with open(script_name, "w") as fout:
            fout.write(out_str)
        os.chmod(script_name, 0o0777)

####


class InferenceOnInjectionBatch():
    def __init__(self, opts, run_dir, inj_exe_name='inspinj', inf_exe_name='inference',
                 plt_exe_name='plot', verbose=False):
        '''
        Thin wrapper class that encapsulates a suite of injection runs

        Parameters
        ----------
        opts : workflow.ConfigParser object
            Options for the workflow
        run_dir : string
            Path to the main directory where all analyses are to be run
        '''
        self.verbose = verbose
        self.opts = opts
        self.inj_exe_name = inj_exe_name
        self.inf_exe_name = inf_exe_name
        self.plt_exe_name = plt_exe_name

        self.num_injections = int(opts.get(self.inj_exe_name, 'ninjections'))
        self.inj_config = opts.get(self.inj_exe_name, 'config-files')

        self.data_configs = opts.get('workflow', 'data').split()
        self.sampler_configs = opts.get('workflow', 'sampler').split()
        self.inf_configs = opts.get('workflow', 'inference').split()

        import itertools
        self.config_combos = list(itertools.product(self.data_configs,
                                                    self.sampler_configs,
                                                    self.inf_configs))
        self.runs = {}
        for inj_num in range(self.num_injections):
            for configs in self.config_combos:
                confs = {}
                confs['injection'] = self.inj_config
                confs['data'], confs['sampler'], confs['inference'] = configs
                run_tag = self.get_run_tag()
                self.runs[run_tag] = InjectionInferenceAnalysis(
                    opts, self.name_run_dir(inj_num, confs), confs,
                    inj_exe_name=self.inj_exe_name,
                    inf_exe_name=self.inf_exe_name,
                    plt_exe_name=self.plt_exe_name,
                    verbose=self.verbose)

    def get_opts(self): return self.opts

    def get_run_dir(self): return self.run_dir

    def setup_runs(self):
        for r in self.runs:
            self.runs[r].setup()

    def get_runs(self): return self.runs

    def get_run_tag(self): return get_unique_hex_tag()

    def name_run_dir(self, inj_num, configs):
        return 'injection{0:03d}/{1}/{2}/{3}'.format(
            inj_num,
            configs['data'].split('.ini')[0],
            configs['sampler'].split('.ini')[0],
            configs['inference'].split('.ini')[0])


####
############################################################
# command line usage
parser = argparse.ArgumentParser(usage=__file__ + " [--options]",
                                 description=__doc__)
parser.add_argument("--version", action="version", version=__version__,
                    help="Prints version information.")
parser.add_argument("--verbose", action="store_true", default=False,
                    help="Print logging messages.")
# workflow options
parser.add_argument("--skip-creating-injections", action="store_true",
                    help="Skip calling lalapps_inspinj and assume injections already exist",
                    default=False)

# output options
parser.add_argument("--output-dir", type=str, required=False, default='',
                    help="Output directory path.")
parser.add_argument("--force", action="store_true", default=False,
                    help="If the output-dir already exists, overwrite it. "
                         "Otherwise, an OSError is raised.")
parser.add_argument("--save-backup", action="store_true",
                    default=False,
                    help="Don't delete the backup file after the run has "
                         "completed.")
# parallelization options
parser.add_argument("--nprocesses", type=int, default=1,
                    help="Number of processes to use. If not given then only "
                         "a single core will be used.")
parser.add_argument("--use-mpi", action='store_true', default=False,
                    help="Use MPI to parallelize the sampler")
parser.add_argument("--samples-file", default=None,
                    help="Use an iteration from an InferenceFile as the "
                         "initial proposal distribution. The same "
                         "number of walkers and the same [variable_params] "
                         "section in the configuration file should be used. "
                         "The priors must allow encompass the initial "
                         "positions from the InferenceFile being read.")
parser.add_argument("--seed", type=int, default=0,
                    help="Seed to use for the random number generator that "
                         "initially distributes the walkers. Default is 0.")
# add config options
configuration.add_workflow_command_line_group(parser)
# add module pre-defined options
fft.insert_fft_option_group(parser)
opt.insert_optimization_option_group(parser)
scheme.insert_processing_option_group(parser)

# parse command line
opts = parser.parse_args()

# setup log
# If we're running in MPI mode, only allow the parent to print
if opts.use_mpi:
    from mpi4py import MPI
    rank = MPI.COMM_WORLD.Get_rank()
    opts.verbose &= rank == 0
pycbc.init_logging(opts.verbose)

# verify options are sane
fft.verify_fft_options(opts, parser)
opt.verify_optimization_options(opts, parser)
scheme.verify_processing_options(opts, parser)

# set seed
numpy.random.seed(opts.seed)
logging.info("Using seed %i", opts.seed)

# we'll silence numpy warnings since they are benign and make for confusing
# logging output
numpy.seterr(divide='ignore', invalid='ignore')

# Sort out the directory where all analyses are to be run
analyses_dir = os.getcwd()
if len(opts.output_dir) > 0:
    analyses_dir = opts.output_dir
if os.path.exists(analyses_dir):
    if not opts.force:
        raise IOError(
            "Output directory {} exists. Use --force to overwrite it.".format(analyses_dir))
os.chdir(analyses_dir)
logging.info("Will setup analyses in {0}".format(analyses_dir))

# get scheme
ctx = scheme.from_cli(opts)
fft.from_cli(opts)


with ctx:

    # read configuration file
    confs = configuration.WorkflowConfigParser.from_cli(opts)

    # Workspace dirs
    logging.info("Making workspace directories")
    for d in ['scripts', 'log', 'plots']:
        mkdir(d)

    # Get logdir
    try:
        log_path = confs.get("workflow", 'log-path')
    except:
        log_path = './'

    tempfile.tempdir = log_path
    tempfile.template = 'pycbc_inference_injections.dag.log.'
    logfile = tempfile.mktemp()

    try:
        accounting_group = confs.get('workflow', 'accounting-group')
    except:
        accounting_group = None
        logging.warn('Warning: accounting-group not specified, LDG clusters may'
                     ' reject this workflow!')

    logging.info("Creating DAG")
    dag = CondorDAG(logfile)

    dag.set_dag_file("pycbc_inference_injections")
    dag.set_dax_file("pycbc_inference_injections")

    analyses = InferenceOnInjectionBatch(confs, analyses_dir, verbose=True)
    # analyses.setup_runs()
    all_injection_runs = analyses.get_runs()

    for tag in all_injection_runs:
        run = all_injection_runs[tag]

        # Setup analysis directory
        run.setup()

        # Configure injection job
        req_mem = None
        if confs.has_option('workflow', 'inspinj-request-memory'):
            req_mem = confs.get('workflow', 'inspinj-request-memory')
        inj_job = InferenceJob('log', run.get_inj_exe_path(), None, None,
                               accounting_group=accounting_group,
                               request_memory=req_mem)
        inj_node = CondorDAGNode(inj_job)
        dag.add_node(inj_node)

        # Configure inference job
        req_mem = None
        if confs.has_option('workflow', 'inference-request-memory'):
            req_mem = confs.get('workflow', 'inference-request-memory')
        inf_job = InferenceJob('log', run.get_inf_exe_path(), None, None,
                               accounting_group=accounting_group,
                               request_memory=req_mem)
        inf_node = CondorDAGNode(inf_job)
        inf_node.add_parent(inj_node)
        dag.add_node(inf_node)

        # Configure plotting job
        req_mem = None
        if confs.has_option('workflow', 'plot-request-memory'):
            req_mem = confs.get('workflow', 'plot-request-memory')
        if confs.has_option('executables', 'plot'):
            plt_job = InferenceJob('log', run.get_plt_exe_path(), None, None,
                                   accounting_group=accounting_group,
                                   request_memory=req_mem)
            plt_node = CondorDAGNode(plt_job)
            plt_node.add_parent(inf_node)
            dag.add_node(plt_node)

    # Write out the DAG
    dag.write_sub_files()
    dag.write_script()
    dag.write_concrete_dag()

logging.info('Done')
