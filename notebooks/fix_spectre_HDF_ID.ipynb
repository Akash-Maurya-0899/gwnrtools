{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import h5py, os, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def add_tensor_component_to_spectre_group(hdf_group, tensors, order_on='connectivity'):\n",
    "    \"\"\"This function takes in a pointer to an open HDF5 file group, within\n",
    "    which data is to be stored. It also takes in a dictionary of tensor\n",
    "    components. The argument `order_on` ensures that the components are \n",
    "    populated in the right order contiguously.\n",
    "    \"\"\"\n",
    "    if order_on is not None:\n",
    "        assert order_on in hdf_group, \"{0} not present in the HDF group provided\".format(order_on)\n",
    "        assert order_on in tensors, \"{0} not present in the tensors provided\".format(order_on)\n",
    "        o1 = hdf_group[order_on][()]\n",
    "        o2 = tensors[order_on]\n",
    "        \n",
    "    for t in tensors:\n",
    "        if t in hdf_group:\n",
    "            continue\n",
    "        hdf_group.create_dataset(t, data=tensors[t])\n",
    "    return hdf_group\n",
    "\n",
    "def combine_element_wise_spectre_data(hdf_group,\n",
    "                                      include='all', exclude=[],\n",
    "                                      combined_tensors={},\n",
    "                                      verbose=True):\n",
    "    \"\"\"This function takes in a pointer to an open HDF5 file group, within\n",
    "    which is data stored in spectre's ExtentsAndTensorVolumeData format, i.e.\n",
    "    in subgroups for each element, there are all tensor components stored\n",
    "    for the same. The optional inputs `include` and `exclude` are lists of\n",
    "    tensors to collate (or not).\n",
    "    \"\"\"\n",
    "    # Use the first element to get a list of tensors and elements\n",
    "    elements = list(hdf_group.keys())\n",
    "    first_elem = elements[0]    \n",
    "    all_tensors = list(hdf_group[first_elem].keys())    \n",
    "    # Include only those tensors that the user specifies in include\n",
    "    if type(include) is list:\n",
    "        all_tensors = list(set(all_tensors).intersection(set(include)))\n",
    "        if len(all_tensors) == 0:\n",
    "            raise IOError(\"Did you want to include any tensors..?\")\n",
    "    elif include == 'all':\n",
    "        pass\n",
    "    else:\n",
    "        raise IOError(\"Either provide a list of tensor components as `include` OR just 'all'\")    \n",
    "    # Exclude tensors the user requests for\n",
    "    assert type(exclude) is list, \"Provide an (empty or filled) list of tensors to exclude\"\n",
    "    all_tensors = list(set(all_tensors).difference(set(exclude)))\n",
    "    # Collate and populate tensors\n",
    "    for t in all_tensors:\n",
    "        if verbose:\n",
    "            print(\"... combining data for {}\".format(t))\n",
    "        if t not in combined_tensors:\n",
    "            combined_tensors[t] = []\n",
    "            for sd in elements:\n",
    "                combined_tensors[t] = np.append(combined_tensors[t], hdf_group[sd][t][()])\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\" ... skipping data for {} as its already present\".format(t))\n",
    "    return combined_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Usage of above methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... combining data for InitialGaugeH_t\n",
      "... combining data for InitialGaugeH_x\n",
      "... combining data for InitialGaugeH_y\n",
      "... combining data for InitialGaugeH_z\n",
      "... combining data for SpacetimeDerivInitialGaugeH_tt\n",
      "... combining data for SpacetimeDerivInitialGaugeH_tx\n",
      "... combining data for SpacetimeDerivInitialGaugeH_ty\n",
      "... combining data for SpacetimeDerivInitialGaugeH_tz\n",
      "... combining data for SpacetimeDerivInitialGaugeH_xt\n",
      "... combining data for SpacetimeDerivInitialGaugeH_xx\n",
      "... combining data for SpacetimeDerivInitialGaugeH_xy\n",
      "... combining data for SpacetimeDerivInitialGaugeH_xz\n",
      "... combining data for SpacetimeDerivInitialGaugeH_yt\n",
      "... combining data for SpacetimeDerivInitialGaugeH_yx\n",
      "... combining data for SpacetimeDerivInitialGaugeH_yy\n",
      "... combining data for SpacetimeDerivInitialGaugeH_yz\n",
      "... combining data for SpacetimeDerivInitialGaugeH_zt\n",
      "... combining data for SpacetimeDerivInitialGaugeH_zx\n",
      "... combining data for SpacetimeDerivInitialGaugeH_zy\n",
      "... combining data for SpacetimeDerivInitialGaugeH_zz\n"
     ]
    }
   ],
   "source": [
    "ct = {}\n",
    "with h5py.File('ID_VolumeData0.h5', 'r') as f:\n",
    "    base = 'element_data.vol'\n",
    "    observation_id = list(f[base].keys())[0]\n",
    "    h = f[base][observation_id]\n",
    "    for t in list(h[list(h.keys())[0]].keys()):\n",
    "        if 'Gauge' not in t:\n",
    "            continue\n",
    "        ct = combine_element_wise_spectre_data(h, include = [t],\n",
    "                                               combined_tensors=ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['InitialGaugeH_t', 'InitialGaugeH_x', 'InitialGaugeH_y', 'InitialGaugeH_z', 'SpacetimeDerivInitialGaugeH_tt', 'SpacetimeDerivInitialGaugeH_tx', 'SpacetimeDerivInitialGaugeH_ty', 'SpacetimeDerivInitialGaugeH_tz', 'SpacetimeDerivInitialGaugeH_xt', 'SpacetimeDerivInitialGaugeH_xx', 'SpacetimeDerivInitialGaugeH_xy', 'SpacetimeDerivInitialGaugeH_xz', 'SpacetimeDerivInitialGaugeH_yt', 'SpacetimeDerivInitialGaugeH_yx', 'SpacetimeDerivInitialGaugeH_yy', 'SpacetimeDerivInitialGaugeH_yz', 'SpacetimeDerivInitialGaugeH_zt', 'SpacetimeDerivInitialGaugeH_zx', 'SpacetimeDerivInitialGaugeH_zy', 'SpacetimeDerivInitialGaugeH_zz'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... combining data for SpacetimeDerivInitialGaugeH_tt\n",
      " ... skipping data for SpacetimeDerivInitialGaugeH_tt as its already present\n",
      "... combining data for SpacetimeDerivInitialGaugeH_tx\n",
      " ... skipping data for SpacetimeDerivInitialGaugeH_tx as its already present\n",
      "... combining data for SpacetimeDerivInitialGaugeH_ty\n",
      " ... skipping data for SpacetimeDerivInitialGaugeH_ty as its already present\n",
      "... combining data for SpacetimeDerivInitialGaugeH_tz\n",
      " ... skipping data for SpacetimeDerivInitialGaugeH_tz as its already present\n",
      "... combining data for SpacetimeDerivInitialGaugeH_xt\n",
      " ... skipping data for SpacetimeDerivInitialGaugeH_xt as its already present\n",
      "... combining data for SpacetimeDerivInitialGaugeH_xx\n",
      " ... skipping data for SpacetimeDerivInitialGaugeH_xx as its already present\n",
      "... combining data for SpacetimeDerivInitialGaugeH_xy\n",
      " ... skipping data for SpacetimeDerivInitialGaugeH_xy as its already present\n",
      "... combining data for SpacetimeDerivInitialGaugeH_xz\n",
      " ... skipping data for SpacetimeDerivInitialGaugeH_xz as its already present\n",
      "... combining data for SpacetimeDerivInitialGaugeH_yt\n",
      " ... skipping data for SpacetimeDerivInitialGaugeH_yt as its already present\n",
      "... combining data for SpacetimeDerivInitialGaugeH_yx\n",
      " ... skipping data for SpacetimeDerivInitialGaugeH_yx as its already present\n",
      "... combining data for SpacetimeDerivInitialGaugeH_yy\n",
      " ... skipping data for SpacetimeDerivInitialGaugeH_yy as its already present\n",
      "... combining data for SpacetimeDerivInitialGaugeH_yz\n",
      " ... skipping data for SpacetimeDerivInitialGaugeH_yz as its already present\n",
      "... combining data for SpacetimeDerivInitialGaugeH_zt\n",
      " ... skipping data for SpacetimeDerivInitialGaugeH_zt as its already present\n",
      "... combining data for SpacetimeDerivInitialGaugeH_zx\n",
      " ... skipping data for SpacetimeDerivInitialGaugeH_zx as its already present\n",
      "... combining data for SpacetimeDerivInitialGaugeH_zy\n",
      " ... skipping data for SpacetimeDerivInitialGaugeH_zy as its already present\n",
      "... combining data for SpacetimeDerivInitialGaugeH_zz\n",
      " ... skipping data for SpacetimeDerivInitialGaugeH_zz as its already present\n",
      "... combining data for SpacetimeMetric_tt\n",
      "... combining data for SpacetimeMetric_tx\n",
      "... combining data for SpacetimeMetric_ty\n",
      "... combining data for SpacetimeMetric_tz\n",
      "... combining data for SpacetimeMetric_xt\n",
      "... combining data for SpacetimeMetric_xx\n",
      "... combining data for SpacetimeMetric_xy\n",
      "... combining data for SpacetimeMetric_xz\n",
      "... combining data for SpacetimeMetric_yt\n",
      "... combining data for SpacetimeMetric_yx\n",
      "... combining data for SpacetimeMetric_yy\n",
      "... combining data for SpacetimeMetric_yz\n",
      "... combining data for SpacetimeMetric_zt\n",
      "... combining data for SpacetimeMetric_zx\n",
      "... combining data for SpacetimeMetric_zy\n",
      "... combining data for SpacetimeMetric_zz\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('ID_VolumeData0.h5', 'r') as f:\n",
    "    base = 'element_data.vol'\n",
    "    observation_id = list(f[base].keys())[0]\n",
    "    h = f[base][observation_id]\n",
    "    for t in list(h[list(h.keys())[0]].keys()):\n",
    "        if 'Spacetime' not in t:\n",
    "            continue\n",
    "        ct = combine_element_wise_spectre_data(h, include = [t],\n",
    "                                               combined_tensors=ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['InitialGaugeH_t', 'InitialGaugeH_x', 'InitialGaugeH_y', 'InitialGaugeH_z', 'SpacetimeDerivInitialGaugeH_tt', 'SpacetimeDerivInitialGaugeH_tx', 'SpacetimeDerivInitialGaugeH_ty', 'SpacetimeDerivInitialGaugeH_tz', 'SpacetimeDerivInitialGaugeH_xt', 'SpacetimeDerivInitialGaugeH_xx', 'SpacetimeDerivInitialGaugeH_xy', 'SpacetimeDerivInitialGaugeH_xz', 'SpacetimeDerivInitialGaugeH_yt', 'SpacetimeDerivInitialGaugeH_yx', 'SpacetimeDerivInitialGaugeH_yy', 'SpacetimeDerivInitialGaugeH_yz', 'SpacetimeDerivInitialGaugeH_zt', 'SpacetimeDerivInitialGaugeH_zx', 'SpacetimeDerivInitialGaugeH_zy', 'SpacetimeDerivInitialGaugeH_zz', 'SpacetimeMetric_tt', 'SpacetimeMetric_tx', 'SpacetimeMetric_ty', 'SpacetimeMetric_tz', 'SpacetimeMetric_xt', 'SpacetimeMetric_xx', 'SpacetimeMetric_xy', 'SpacetimeMetric_xz', 'SpacetimeMetric_yt', 'SpacetimeMetric_yx', 'SpacetimeMetric_yy', 'SpacetimeMetric_yz', 'SpacetimeMetric_zt', 'SpacetimeMetric_zx', 'SpacetimeMetric_zy', 'SpacetimeMetric_zz'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... combining data for Pi_tt\n",
      "... combining data for Pi_tx\n",
      "... combining data for Pi_ty\n",
      "... combining data for Pi_tz\n",
      "... combining data for Pi_xt\n",
      "... combining data for Pi_xx\n",
      "... combining data for Pi_xy\n",
      "... combining data for Pi_xz\n",
      "... combining data for Pi_yt\n",
      "... combining data for Pi_yx\n",
      "... combining data for Pi_yy\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('ID_VolumeData0.h5', 'r') as f:\n",
    "    base = 'element_data.vol'\n",
    "    observation_id = list(f[base].keys())[0]\n",
    "    h = f[base][observation_id]\n",
    "    for t in list(h[list(h.keys())[0]].keys()):\n",
    "        if 'Pi' not in t:\n",
    "            continue\n",
    "        ct = combine_element_wise_spectre_data(h, include = [t],\n",
    "                                               combined_tensors=ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ct.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5py' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f3b2262bdba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ID_VolumeData0.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'element_data.vol'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mobservation_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h5py' is not defined"
     ]
    }
   ],
   "source": [
    "with h5py.File('ID_VolumeData0.h5', 'r') as f:\n",
    "    base = 'element_data.vol'\n",
    "    observation_id = list(f[base].keys())[0]\n",
    "    h = f[base][observation_id]\n",
    "    for t in list(h[list(h.keys())[0]].keys()):\n",
    "        if 'Phi' not in t:\n",
    "            continue\n",
    "        ct = combine_element_wise_spectre_data(h, include = [t],\n",
    "                                               combined_tensors=ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e80f7505d496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ct' is not defined"
     ]
    }
   ],
   "source": [
    "ct.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"cp -rv EvolutionVolume0.h5 spectre_ID_VolumeData0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"spectre_ID_VolumeData0.h5\", \"a\") as f:\n",
    "    base = 'element_data.vol'\n",
    "    observation_id = list(f[base].keys())[0]\n",
    "    h = f[base][observation_id]\n",
    "    add_tensor_component_to_spectre_group(h, ct, order_on=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('cp -v {0} {1}'.format('spectre_ID_VolumeData0.h5',\n",
    "                                 id_file.replace('ID_VolumeData0.h5', '')\n",
    "                                 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Development**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "base = 'element_data.vol'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "id_dir = '/home/prayush.kumar/wheeler_scratch/projects/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "id_file = id_dir + \\\n",
    "    'BoundaryConditionsGeneralizedHarmonic/gh_bctest_3d/perturbed_kerrschild/ID_VolumeData0.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "export_coords_file = id_dir + \\\n",
    "    'BoundaryConditionsGeneralizedHarmonic/gh_bctest_3d/perturbed_kerrschild/VolumeData0.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('cp -v {} .'.format(id_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('cp -v {} .'.format(export_coords_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"cp -rv VolumeData0.h5 spectre_ID_VolumeData0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "f = h5py.File('spectre_ID_VolumeData0.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetInvJacobian(Logical,Inertial) (2239488,)\n",
      "InertialCoordinates_x (2239488,)\n",
      "InertialCoordinates_y (2239488,)\n",
      "InertialCoordinates_z (2239488,)\n",
      "connectivity (12582912,)\n",
      "grid_names (70016,)\n",
      "total_extents (9216,)\n"
     ]
    }
   ],
   "source": [
    "observation_id = list(f[base].keys())[0]\n",
    "for k in f[base][observation_id].keys():\n",
    "    print(k, f[base][observation_id][k][()].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f[base][observation_id].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "File created by `ExportCoordinates3D` contains 7 datasets. These include the coordinates, `connectivity` and `total_extents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([91, 66, 52, 55, 44, 40, 76, 50, 73, 51, 44, 76, 50, 73, 51, 44, 76,\n",
       "       50, 73, 51, 41, 93, 58, 91, 66, 52, 55, 44, 40, 76, 50, 73, 51, 44,\n",
       "       76, 50, 73, 51, 44, 76, 50, 73, 50, 41, 93, 58, 91, 66, 52, 55, 44,\n",
       "       40, 76, 50, 73, 51, 44, 76, 50, 73, 51, 44, 76, 50, 73, 49, 41, 93,\n",
       "       58, 91, 66, 52, 55, 44, 40, 76, 50, 73, 51, 44, 76, 50, 73, 50, 44,\n",
       "       76, 50, 73, 50, 41, 93, 58, 91, 66, 52, 55, 44, 40, 76, 50],\n",
       "      dtype=int8)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[base][observation_id]['grid_names'][()][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9], dtype=uint64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[base][observation_id]['total_extents'][()][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,  10,   9,  81,  82,  91,  90,  81,  82,  91,  90, 162,\n",
       "       163, 172, 171, 162, 163, 172, 171, 243, 244, 253, 252, 243, 244,\n",
       "       253, 252, 324, 325, 334, 333, 324, 325, 334, 333, 405, 406, 415,\n",
       "       414, 405, 406, 415, 414, 486, 487, 496, 495, 486, 487, 496, 495,\n",
       "       567, 568, 577, 576, 567, 568, 577, 576, 648, 649, 658, 657,   9,\n",
       "        10,  19,  18,  90,  91, 100,  99,  90,  91, 100,  99, 171, 172,\n",
       "       181, 180, 171, 172, 181, 180, 252, 253, 262, 261, 252, 253, 262,\n",
       "       261, 333, 334, 343, 342, 333, 334, 343, 342], dtype=int32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[base][observation_id]['connectivity'][()][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "id_f = h5py.File('spectre_ID_VolumeData0.h5', 'r')\n",
    "observation_id = list(id_f[base].keys())[0]\n",
    "id_h = id_f[base][observation_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now, we examine the ID file `ID_VolumeData0.h5` generated by Geoffrey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "gl_f = h5py.File('ID_VolumeData0.h5', 'r')\n",
    "observation_id = list(gl_f[base].keys())[0]\n",
    "g = gl_f[base]\n",
    "gl_h = gl_f[base][observation_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[B0,(L2I0,L2I0,L2I0)]'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(gl_h.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['observation_value']>\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(gl_h.attrs.keys())\n",
    "print(gl_h.attrs['observation_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gl_h.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "There are clearly 3072 subdomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['InertialCoordinates_x', 'InertialCoordinates_y', 'InertialCoordinates_z', 'InitialGaugeH_t', 'InitialGaugeH_x', 'InitialGaugeH_y', 'InitialGaugeH_z', 'Phi_xtt', 'Phi_xtx', 'Phi_xty', 'Phi_xtz', 'Phi_xxt', 'Phi_xxx', 'Phi_xxy', 'Phi_xxz', 'Phi_xyt', 'Phi_xyx', 'Phi_xyy', 'Phi_xyz', 'Phi_xzt', 'Phi_xzx', 'Phi_xzy', 'Phi_xzz', 'Phi_ytt', 'Phi_ytx', 'Phi_yty', 'Phi_ytz', 'Phi_yxt', 'Phi_yxx', 'Phi_yxy', 'Phi_yxz', 'Phi_yyt', 'Phi_yyx', 'Phi_yyy', 'Phi_yyz', 'Phi_yzt', 'Phi_yzx', 'Phi_yzy', 'Phi_yzz', 'Phi_ztt', 'Phi_ztx', 'Phi_zty', 'Phi_ztz', 'Phi_zxt', 'Phi_zxx', 'Phi_zxy', 'Phi_zxz', 'Phi_zyt', 'Phi_zyx', 'Phi_zyy', 'Phi_zyz', 'Phi_zzt', 'Phi_zzx', 'Phi_zzy', 'Phi_zzz', 'Pi_tt', 'Pi_tx', 'Pi_ty', 'Pi_tz', 'Pi_xt', 'Pi_xx', 'Pi_xy', 'Pi_xz', 'Pi_yt', 'Pi_yx', 'Pi_yy', 'Pi_yz', 'Pi_zt', 'Pi_zx', 'Pi_zy', 'Pi_zz', 'SpacetimeDerivInitialGaugeH_tt', 'SpacetimeDerivInitialGaugeH_tx', 'SpacetimeDerivInitialGaugeH_ty', 'SpacetimeDerivInitialGaugeH_tz', 'SpacetimeDerivInitialGaugeH_xt', 'SpacetimeDerivInitialGaugeH_xx', 'SpacetimeDerivInitialGaugeH_xy', 'SpacetimeDerivInitialGaugeH_xz', 'SpacetimeDerivInitialGaugeH_yt', 'SpacetimeDerivInitialGaugeH_yx', 'SpacetimeDerivInitialGaugeH_yy', 'SpacetimeDerivInitialGaugeH_yz', 'SpacetimeDerivInitialGaugeH_zt', 'SpacetimeDerivInitialGaugeH_zx', 'SpacetimeDerivInitialGaugeH_zy', 'SpacetimeDerivInitialGaugeH_zz', 'SpacetimeMetric_tt', 'SpacetimeMetric_tx', 'SpacetimeMetric_ty', 'SpacetimeMetric_tz', 'SpacetimeMetric_xt', 'SpacetimeMetric_xx', 'SpacetimeMetric_xy', 'SpacetimeMetric_xz', 'SpacetimeMetric_yt', 'SpacetimeMetric_yx', 'SpacetimeMetric_yy', 'SpacetimeMetric_yz', 'SpacetimeMetric_zt', 'SpacetimeMetric_zx', 'SpacetimeMetric_zy', 'SpacetimeMetric_zz', 'connectivity']\n"
     ]
    }
   ],
   "source": [
    "print(list(gl_h[kk].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "num_tensor_comps = [len(gl_h[k]) for k in list(gl_h.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(np.max(num_tensor_comps), np.min(num_tensor_comps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "There are 104 tensor components etc for each subdomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0,   1,  10,   9,  81,  82,  91,  90,  81,  82,  91,  90, 162,\n",
       "       163, 172, 171, 162, 163, 172, 171, 243, 244, 253, 252, 243, 244,\n",
       "       253, 252, 324, 325, 334, 333, 324, 325, 334, 333, 405, 406, 415,\n",
       "       414, 405, 406, 415, 414, 486, 487, 496, 495, 486, 487, 496, 495,\n",
       "       567, 568, 577, 576, 567, 568, 577, 576, 648, 649, 658, 657,   9,\n",
       "        10,  19,  18,  90,  91, 100,  99,  90,  91, 100,  99, 171, 172,\n",
       "       181, 180, 171, 172, 181, 180, 252, 253, 262, 261, 252, 253, 262,\n",
       "       261, 333, 334, 343, 342, 333, 334, 343, 342], dtype=int32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk = list(gl_h.keys())[0]\n",
    "print(gl_h[kk]['connectivity'][()].shape)\n",
    "gl_h[kk]['connectivity'][()][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,  10,   9,  81,  82,  91,  90,  81,  82,  91,  90, 162,\n",
       "       163, 172, 171, 162, 163, 172, 171, 243, 244, 253, 252, 243, 244,\n",
       "       253, 252, 324, 325, 334, 333, 324, 325, 334, 333, 405, 406, 415,\n",
       "       414, 405, 406, 415, 414, 486, 487, 496, 495, 486, 487, 496, 495,\n",
       "       567, 568, 577, 576, 567, 568, 577, 576, 648, 649, 658, 657,   9,\n",
       "        10,  19,  18,  90,  91, 100,  99,  90,  91, 100,  99, 171, 172,\n",
       "       181, 180, 171, 172, 181, 180, 252, 253, 262, 261, 252, 253, 262,\n",
       "       261, 333, 334, 343, 342, 333, 334, 343, 342], dtype=int32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_h['connectivity'][()][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gl_n_elems = len(gl_h.keys())\n",
    "gl_conn_per_elem = gl_h[kk]['connectivity'][()].shape[0]\n",
    "id_n_conn = id_h['connectivity'][()].shape[0]\n",
    "\n",
    "gl_n_elems * gl_conn_per_elem == id_n_conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The product of `number of elements` and `no of connectivity entries for each element` should be equal to the `connectivity` entries in Geoffrey's data. It is, as it turns out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True\n",
      "1 False\n",
      "2 False\n",
      "3 False\n",
      "4 False\n",
      "5 False\n",
      "6 False\n",
      "7 False\n",
      "8 False\n",
      "9 False\n",
      "10 False\n",
      "11 False\n",
      "12 False\n",
      "13 False\n",
      "14 False\n",
      "15 False\n",
      "16 False\n",
      "17 False\n",
      "18 False\n",
      "19 False\n",
      "20 False\n",
      "21 False\n"
     ]
    }
   ],
   "source": [
    "all_tvals = []\n",
    "for ii in range(gl_n_elems):\n",
    "    kk = list(gl_h.keys())[ii]\n",
    "    tval = np.all(gl_h[kk]['connectivity'][()][:] == id_h['connectivity'][()][ii*4096 : (ii+1)*4096])\n",
    "    all_tvals.append(tval)\n",
    "    print(ii, tval)\n",
    "    if ii > 20: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**The problem now is to match the connectivity entries from element-wise data to collated data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using element: [B1,(L2I2,L2I3,L2I2)]\n"
     ]
    }
   ],
   "source": [
    "test_ii = 110\n",
    "\n",
    "# first, get gl data for this subdomain\n",
    "kk = list(gl_h.keys())[test_ii]\n",
    "print(\"using element: {}\".format(kk))\n",
    "gl_test_d = gl_h[kk]['connectivity'][()][:]\n",
    "\n",
    "# then, get total data\n",
    "id_test_d = id_h['connectivity'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_test_d.shape[0] // gl_test_d.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "all_tvals = []\n",
    "for jj in range(gl_n_elems):\n",
    "    tval = np.all(gl_test_d == id_test_d[jj*4096 : (jj+1)*4096])\n",
    "    all_tvals.append(tval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]),)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(all_tvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Apparently, all connectivity info is identical between elements in GL data!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Now, we work with the inertial coordinates. Those have to be mapped between data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using element: [B0,(L2I0,L2I0,L2I0)]\n"
     ]
    }
   ],
   "source": [
    "test_ii = 0\n",
    "\n",
    "# first, get gl data for this subdomain\n",
    "kk = list(gl_h.keys())[test_ii]\n",
    "print(\"using element: {}\".format(kk))\n",
    "gl_test_d = gl_h[kk]['InertialCoordinates_x'][()][:]\n",
    "\n",
    "# then, get total data\n",
    "id_test_d = id_h['InertialCoordinates_x'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/ligo-containers.opensciencegrid.org/lscsoft/conda/latest/envs/ligo-py36/lib/python3.6/site-packages/ipykernel/__main__.py:3: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "all_tvals = []\n",
    "for jj in range(gl_n_elems):\n",
    "    tval = np.all(gl_test_d == id_test_d[jj*4096 : (jj+1)*4096])\n",
    "    all_tvals.append(tval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(all_tvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "id_f.close()\n",
    "gl_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Geoffrey's script to add soft links for the symmetric\n",
    "# components of tensors in volume data file\n",
    "\n",
    "import argparse\n",
    "import h5py\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    help = \"\"\"Takes a spectre-volume-data-format HDF5 file\n",
    "    that contains SpacetimeMetric, Pi, and Phi, and makes\n",
    "    soft links for off-diagonal components. This script\n",
    "    assumes that the data in the HDF5 file consits of\n",
    "    the spacetime components tt, tx, ty, tz, xx, xy, xz, yy, yz, zz.\n",
    "    The soft links map xt -> tx, yx -> xy, etc.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=help)\n",
    "    parser.add_argument('--input', required=True,\n",
    "                        help = \"\"\"Name of HDF5 file\"\"\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spacetime_components = [\"t\", \"x\", \"y\", \"z\"]\n",
    "    symmetric_tensors_in_file = [\"Phi_x\",  \"Phi_y\", \"Phi_z\", \"Pi_\",\n",
    "                                 \"SpacetimeMetric_\"]\n",
    "\n",
    "    file = h5py.File(args.input, 'a')\n",
    "    base = \"element_data.vol\"\n",
    "    for obs_id in file[base].keys():\n",
    "        for tensor in symmetric_tensors_in_file:\n",
    "            for i,comp1 in enumerate(spacetime_components):\n",
    "                for j,comp2 in enumerate(spacetime_components):\n",
    "                    if i > j:\n",
    "                        file[base][obs_id][tensor+comp1+comp2] = \\\n",
    "                          h5py.SoftLink(\"/\" + base + \"/\" + obs_id + \"/\"\n",
    "                                        + tensor + comp2 + comp1)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import h5py\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "def rename_variables(legend_line):\n",
    "    \"\"\"Takes a string containing space-delimited tensor names and\n",
    "    returns the restring with some tensors renamed:\n",
    "        psi, kappa, H and GradH (i.e., flattened SpacetimeDerivH)\n",
    "        ->\n",
    "        SpacetimeMetric, Pi/Phi, GaugeH, and SpacetimeDerivGaugeH\n",
    "    \"\"\"\n",
    "    legend_line = legend_line.replace('psi', 'SpacetimeMetric_')\n",
    "    legend_line = legend_line.replace('kappat', 'Pi_')\n",
    "    legend_line = legend_line.replace('kappax', 'Phi_x').replace('kappay', 'Phi_y').replace('kappaz', 'Phi_z')\n",
    "    legend_line = legend_line.replace('GradH', 'SpacetimeDerivInitialGaugeH_')\n",
    "    legend_line = legend_line.replace(' H', ' InitialGaugeH_')\n",
    "    return legend_line.rstrip()\n",
    "\n",
    "def get_spec_points(spectre_file):\n",
    "    \"\"\"Converts points from a file produced by spectre's ExportCoordinates\n",
    "    to a flat list of points, suitable for SpEC to read in (e.g.,\n",
    "    for interpolating data onto those points)\"\"\"\n",
    "    observation_id = list(spectre_file['element_data.vol'].keys())[0]\n",
    "    coords_dict = dict(spectre_file['element_data.vol'][observation_id])\n",
    "\n",
    "    components = ['InertialCoordinates_x', 'InertialCoordinates_y', 'InertialCoordinates_z']\n",
    "    dim = len(components)\n",
    "\n",
    "    coords = []\n",
    "    for component in components:\n",
    "        coords.append([])\n",
    "\n",
    "    for i,component in enumerate(components):\n",
    "        coords[i].append(coords_dict[component])\n",
    "    return np.transpose(np.array([np.concatenate(x) for x in coords]))\n",
    "\n",
    "def write_spec_points_file(spectre_points_filename, spec_points_filename):\n",
    "    \"\"\"Read the coordinates from a spectre domain and write them in a\n",
    "    file readable by spec's InterpolateToSpecifiedPoints. The input arguments\n",
    "    are the name of a spectre points file (e.g. 'VolumeData0.h5') and\n",
    "    the name of the spec points file to write (e.g. 'PointsList.txt').\"\"\"\n",
    "    spectre_file = h5py.File(spectre_points_filename, 'r')\n",
    "    points = get_spec_points(spectre_file)\n",
    "    spectre_file.close()\n",
    "    np.savetxt(spec_points_filename, points)\n",
    "\n",
    "def insert_spec_data(spectre_points_filename, spec_data_filename):\n",
    "    \"\"\"Insert tensor data given in the spectre volume-data file\n",
    "    named spec_data_filename (typically output by spec's\n",
    "    InterpolateToSpecifiedPoints) into the spectre volume-data file named\n",
    "    spectre_points_filename. Note that this function will rename some\n",
    "    variables if it finds them by calling rename_variables()\"\"\"\n",
    "    # Read the interpolated data into a numpy array\n",
    "    data_to_insert = np.genfromtxt(spec_data_filename)\n",
    "\n",
    "    # Get the legend\n",
    "    spec_file = open(spec_data_filename, 'r')\n",
    "    lines = spec_file.readlines()\n",
    "\n",
    "    # spec output lists the components as a comment on the second line\n",
    "    # in the format '# psitt psitx ...'\n",
    "    legend_line = lines[1][2:]\n",
    "    legend_line = rename_variables(legend_line)\n",
    "    legend = legend_line.split(\" \")\n",
    "\n",
    "    legend_dict = {}\n",
    "    for i, key in enumerate(legend):\n",
    "        legend_dict[key] = i\n",
    "    spec_file.close()\n",
    "\n",
    "    # Open file read-only to determine observation_id\n",
    "    spectre_file = h5py.File(spectre_points_filename, 'r')\n",
    "    observation_id = list(spectre_file['element_data.vol'].keys())[0]\n",
    "    spectre_file.close()\n",
    "\n",
    "    # Open file ready to append data\n",
    "    output_file = h5py.File(spectre_points_filename, 'a')\n",
    "\n",
    "    # Loop over keys\n",
    "    for key in legend_dict:\n",
    "        print(\"Inserting \" + key)\n",
    "        spec_data = data_to_insert[:, legend_dict[key]]\n",
    "        output_file['element_data.vol'][observation_id][key] = spec_data\n",
    "\n",
    "    output_file.close()\n",
    "    return legend_dict, data_to_insert\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    help = \"\"\"Convert data between spec and spectre. This script reads\n",
    "    the spectre domain from a spectre volume-data file specified by\n",
    "    --spectre-points-filename, output e.g. by\n",
    "    spectre's ExportCoordinates3D executable. If the option\n",
    "    --output-spec-points-filename is given, this script will write the\n",
    "    spectre grid coordinates into a file to be read by spec's\n",
    "    InterpolateToSpecifiedPoints. If the option\n",
    "    --spec-data-to-insert-filename is given, this script will read data\n",
    "    from the specified file, which contains output from spec's\n",
    "    InterpolateToSpecifiedPoints (with option DumpAllDataIntoSingleFile=yes).\n",
    "    Then, this script will insert that data into a copy of the spectre\n",
    "    volume-data file specified by --output-spectre-points-filename.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=help)\n",
    "    parser.add_argument('--spectre-points-filename', required=True,\n",
    "                        help = \"\"\"Name of spectre volume data file\n",
    "                        containing the x,y,z coordinates of a\n",
    "                        spectre domain.\"\"\")\n",
    "    parser.add_argument('--output-spec-points-filename', required=False,\n",
    "                        help = \"\"\"If specified, output coordinates in spec\n",
    "                        format to this file\"\"\")\n",
    "    parser.add_argument('--spec-data-to-insert-filename', required=False,\n",
    "                        help = \"\"\"If specified, insert data from this file\n",
    "                        into the spectre volume data file given by\n",
    "                        --output-spectre-points-filename (if specified).\"\"\")\n",
    "    parser.add_argument('--output-spectre-points-filename', required=False,\n",
    "                        help = \"\"\"If --spec-data-to-insert-filename is\n",
    "                        specified and this option is specified,\n",
    "                        copy the file given by --spectre-points-filename\n",
    "                        to a new file with name\n",
    "                        --output-spectre-points-filename, and then insert\n",
    "                        the data contained in the file given by\n",
    "                        --spec-data-to-insert-filename.\"\"\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.output_spec_points_filename is not None:\n",
    "        write_spec_points_file(args.spectre_points_filename,\n",
    "                               args.output_spec_points_filename)\n",
    "\n",
    "    if args.spec_data_to_insert_filename is not None:\n",
    "        print(\"Inserting data from \" + args.spec_data_to_insert_filename)\n",
    "        if args.output_spectre_points_filename is not None:\n",
    "            print(\"Inserting into \" + args.output_spectre_points_filename)\n",
    "            shutil.copyfile(args.spectre_points_filename,\n",
    "                            args.output_spectre_points_filename)\n",
    "            insert_spec_data(args.output_spectre_points_filename,\n",
    "                             args.spec_data_to_insert_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ligo-py36",
   "language": "python",
   "name": "ligo-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
